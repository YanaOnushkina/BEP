{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_csv = pd.read_csv('/Volumes/Mac/BepDataResNet/ManuallyAnotatedFileList/validation.csv', sep = ',', names = [\"File_path\",'Face_x', 'Face-y', \"Face_width\", \"Face_height\",\n",
    "                                                                   'Facial_landmarks', 'Expression', 'Valence', 'Arousal'], index_col = \"File_path\")\n",
    "\n",
    "training_csv = pd.read_csv('/Volumes/Mac/BepDataResNet/ManuallyAnotatedFileList/training.csv', sep = ',', names = [\"File_path\",'Face_x', 'Face-y', \"Face_width\", \"Face_height\",\n",
    "                                                                   'Facial_landmarks', 'Expression', 'Valence', 'Arousal'], index_col= \"File_path\", header = 1)\n",
    "\n",
    "training_csv['Expression'] = pd.to_numeric(training_csv['Expression'])\n",
    "validation_csv['Expression'] = pd.to_numeric(validation_csv['Expression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles\n",
    "\n",
    "#found_image_paths = getListOfFiles(\"/Volumes/Mac/BepDataResNet/ManuallyAnnotated\")\n",
    "#train_and_val = pd.concat([training_csv, validation_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data size (3, 256, 256, 3) (3, 11)\n",
      "Train Data size (194, 256, 256, 3) (194, 11)\n",
      "There have been 2 failures\n",
      "The class distirbution is: {0: 0.15, 1: 0.51, 2: 0.25, 3: 0.1, 4: 0.05, 5: 0.0, 6: 0.17, 7: 0.07, 8: 0.29, 9: 0.27, 10: 0.91}\n",
      "The function took 70.46 seconds\n"
     ]
    }
   ],
   "source": [
    "def read_data(input_dir = \"/Volumes/Mac/BepDataResNet/ManuallyAnnotated\",\n",
    "                   output_dir = \"/Users/blazejmanczak/Desktop\"):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #\"\"\"\n",
    "    validation_csv = pd.read_csv('/Volumes/Mac/BepDataResNet/ManuallyAnotatedFileList/validation.csv', sep = ',', names = [\"File_path\",'Face_x', 'Face-y', \"Face_width\", \"Face_height\",\n",
    "                                                                   'Facial_landmarks', 'Expression', 'Valence', 'Arousal'], index_col = \"File_path\")\n",
    "\n",
    "    training_csv = pd.read_csv('/Volumes/Mac/BepDataResNet/ManuallyAnotatedFileList/training.csv', sep = ',', names = [\"File_path\",'Face_x', 'Face-y', \"Face_width\", \"Face_height\",\n",
    "                                                                   'Facial_landmarks', 'Expression', 'Valence', 'Arousal'], index_col= \"File_path\", header = 1)\n",
    "\n",
    "    #\"\"\"\n",
    "    training_csv['Expression'] = pd.to_numeric(training_csv['Expression'])\n",
    "    validation_csv['Expression'] = pd.to_numeric(validation_csv['Expression'])\n",
    "    \n",
    "    train_and_val = pd.concat([training_csv, validation_csv])\n",
    "    train_and_val['RowNumber'] = range(1, train_and_val.shape[0]+1)\n",
    "\n",
    "    found_image_paths = getListOfFiles(input_dir)\n",
    "    \n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    failures = 0 # keeping track of failed imports\n",
    "    class_count = dict(zip(range(0,11), [0]*11))# keep track of label counts\n",
    "\n",
    "    for path in found_image_paths[1:200]:\n",
    "\n",
    "        try: # image is avaliable\n",
    "            \n",
    "            path_for_csv =  path.split(\"Manually_Annotated_Images\")[1][1:]\n",
    "            label_and_rowNum = train_and_val.loc[path_for_csv, [\"Expression\", \"RowNumber\"]]\n",
    "            \n",
    "            img = load_img(path, target_size = (256,256))\n",
    "            x = img_to_array(img)\n",
    "\n",
    "            if label_and_rowNum[1] > training_csv.shape[0]:  # faster way of establishing whether row comes from validation set\n",
    "                X_test.append(x)\n",
    "                Y_test.append(label_and_rowNum[0])\n",
    "            else:\n",
    "                X_train.append(x)\n",
    "                Y_train.append(label_and_rowNum[0])\n",
    "                \n",
    "            class_count[label_and_rowNum[0]] += 1 # keep track of label counts\n",
    "\n",
    "        except: \n",
    "            failures += 1\n",
    "\n",
    "    \n",
    "    X_train = np.asarray(X_train).astype('float16') / 255. # scaling the images by 255\n",
    "    Y_train = keras.utils.to_categorical(Y_train, num_classes = 11)\n",
    "    \n",
    "    X_test = np.asarray(X_test).astype('float16') / 255. # scaling the images by 255\n",
    "    Y_test = keras.utils.to_categorical(Y_test, num_classes = 11)\n",
    "  \n",
    "    np.savez_compressed(\"/Volumes/Mac/BepDataResNet/trainData\", X_train = X_train, Y_train = Y_train)\n",
    "    np.savez_compressed(\"/Volumes/Mac/BepDataResNet/testData\", X_test = X_test, Y_test = Y_test)\n",
    "    \n",
    "    sum_all_vals =  sum(class_count.values())\n",
    "    for key in class_count.keys():\n",
    "        class_count[key] = round(class_count[key] / sum_all_vals), 2)\n",
    "        \n",
    "    print (\"Test Data size\", X_test.shape, Y_test.shape)\n",
    "    print (\"Train Data size\", X_train.shape, Y_train.shape)\n",
    "    print(\"There have been {} failures\".format(failures))\n",
    "    print(\"The class distirbution is:\", class_count)\n",
    "    print(\"The function took\", round(time.time() - start,2), \"seconds\")\n",
    "    \n",
    "\n",
    "read_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
